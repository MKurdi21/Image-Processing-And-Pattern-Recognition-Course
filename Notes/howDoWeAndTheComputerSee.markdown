# How Do We and the Computer See?

## The Human Visual System

In order to understand how to deal with images, we have to understand what seeing is and how it works. 

Humans see when the reflection of light bounces off of an object and falls on the retina(the bundle of nerve sensors in the eye).
A lens controls the focus and sharpness of the image. Rods in the retina detect illumination levels while Cones detect 
colors(there are around 10x more rods than cones). The Distance between the lens and the retina is constant. To focus,
the lens changes shape by either being squished or flattened, changing the focal length and thus the sharpness.
When light falls onto the retina, electrical impulses are created and are sent to the brain via the optic nerve. The Image is 
upside down, since there is only 1 lens.

![Diagram of the Human Eye](https://nei.nih.gov/sites/default/files/nehep-images/eyediagram.gif)

## The Computer
- Images can be produced using different regions of the Electromagnetic Spectrum(x-rays,microwave,MRI,
photography,etc)

### Displaying Images

- Images are displayed as a discrete set of intensities.

- The Eye's ability to discriminate between different intensity levels is an 
important consideration in presenting image processing results. The Main goal 
of image processing is to improve images. The Final judge is the human. Therefore 
it is very important to consider the human eye when 

- The Human visual system can perceive around 10^10 different light intensity levels.

- However, at any one time we can discriminate between a much smaller number of intensities
ie the current sensitivity of the visual system.

- Perceived intensity of a region is related to the light intensities of the regions around it. 
Perceived brightness is not a simple function of intensity. The Visual system tends to overshoot 
or undershoot around the boundary of regions of different intensities. 
  - One phenomena to demonstrate this is the mach band,where we see "concaveness" around the boundaries between the stripes even though 
  they are of constant brightness
  
  ![mach band](http://www.ucalgary.ca/pip369/files/pip369/MachBands.jpg) 


  - Another Phenomena is simultaneous contrast, 
    where a color seems lighter or darker depending on the surrounding region
    
   ![simeltaneeous contrast](http://www.johnpaulcaponigro.com/blog/http://www.johnpaulcaponigro.com/blog/wp-content/themes/zinfandel-blue-10/images/lightdark.jpg)

  - Yet Another Phenomena is optical illusions, where objects seem larger, smaller, moving, or cause us 
    to perceive things that aren't there because of how the visual system works(and fills in stuff that isnt there sometimes). An example is this 
    trippy picture.
    
   ![trippy picture](http://www.optics4kids.org/osa.o4k/media/optics4kids/snakes.gif?width=400&height=388&ext=.gif)

### Light and the Electromagnetic spectrum
- Light is a (very small) part of the Electromagnetic Spectrum that can be sensed by the human eye(400nm to 700nm). 

- The Electromagnetic Spectrum is split up according to wavelengths of different forms of energy.

- This was discovered in 1666 by [Issac Newton](http://www.thestargarden.co.uk/Newtons-theory-of-light.html) when he noticed that light split into a spectrum 
after passing through a piece of glass.

- Humans perceive color based on the light reflected from an object. A green object for example, absorbs
everything except green, so we see it as green.

- Light That has no color is called *monochromatic* or *achromatic*.

  - Its only attribute is intensity.
  
  - Because monochromatic light is perceived from black to white with a 
    set of grays between , we refer to it using *gray level*.
    - Intensity level and gray level mean the same thing.
  
- Light that has color is called *chromatic*.

  - Its attributes in addition to the frequency of the light wave(which gives us the color) are :  
    
    - Radiance : The Energy reflected from the object.
    
    - Luminance : The Energy perceived by the observer.
    
    - Brightness : A Subjective property that is hard to measure but gives the notion of intensity.
    
### Image Acquisition

- Image Acquisition is a problem domain

- Most of the images we are interested in are generated by the combination of 
an "illumination" source and a reflection or absorption of the energy from a 
source by the clements of the "scene" being imaged ie, to create an image, we need :

  - A Source of illumination.
  
  - Devices to collect the energy reflected from or transmitted through the objects.
  
- Radiance is the total energy that flows from the light source.

- Luminance is the level of energy perceived from the light source.

- The Fundamental Limit of Image Acquisition :

> To See an Object, The Electromagnetic Wavelength Must Be No Bigger Than the Object.
 
### Sensing
there are three arrangements of sensors :

1. Single Imaging Sensors : Just a singular sensor on its own. an Example
of its usage is for IR data transmission.

2. Line Imagining Sensors : A Group of sensors in a Line. An example of their usage is in Flatbed Scanners and Bar code readers.

3. Array Sensors : A Group Sensors in an array Used in DSLR or Phone Cameras(Square) or 
MRI(Round).
 
In all cases, incoming light lands on a sensor material responsive 
to that type of energy, generating a voltage.
for details on how images are generated from a physical property, see the
[image formation model](http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/MARBLE/low/fundamentals/models.htm).
it can be summarized by

> f(x,y)=i(x,y)+r(x,y)

where ```i``` is the *illumination*, the amount of light that falls on scene,
and ```r``` is the *reflectance*, the amount of light reflected off the object in the scene.

- Sensors used in acquisition produce a continuous voltage signal. A Digital 
sensor can only measure a limited number of samples at a discrete set of energy 
levels.

- A Digital form of an image is produced through two processes :

  - Sampling(Digitization) : digitizing the intensity of spatial coordinates x,y.
  
  - Quantization(Discretization) : converting the intensity of x,y into discrete levels.

### Representing Images

#### What is an Image to the Computer?

- Images are represented as matrices of values. These values can be any data type

- For Grayscale images, any value of (x,y) in the matrix has a value between 0-255(usually).

  - The Number of grayscale levels is 2^n.
  
  - We can also come up with our own values for intensities. Say we want to represent an image
	with 3 colors only, we will only need 3 values instead of 256.

  -	This is an example of a simple grayscale image and its representation :
	
	![this is an example](https://i.stack.imgur.com/A6g0y.jpg)
- For colored images, any value of (x,y) in the matrix is a vector of red,green,and blue, each 
with a value between 0-255(usually).

- Intensity Level Resolution : Number of intensity levels used to represent the image
  
  - The higher the intensity levels used, the finer level of detail.
  
  - Intensity Level Resolution is given in terms of the number of bits needed
	to store every intensity level.
	
- The Number of bits required to store a digitized image is :

> M\*N\*k  
  
  or

> (x\*y\*Intensity Level Resolution)

- Noise Level : Noise is the grainy texture pattern in images. This is effected by 
multiple factors such as ISO, Blur, contrast, etc. We must be mindful 
of what might cause it and try to reduce it to an acceptable level for
our use. Its **not** a measured value, but rather a phenomenon in 
imagery.

#### Image Properties

- Resolution : Resolution is how much detail we can have
in an image. It depends on sampling and gray level.

  - The Higher the *Sampling Rate*
	and the larger the grayscale value domain, the better the approximation
	of the digitized image is from the original.
	
  - The more the quantization levels, the larger the size of the image.

  - Key Questions to ask when determining resolution : 
    
    - Does the image look aesthetically pleasing?
    
    - Can you see what you need to see in the image?

- Spatial Resolution is determined by how fine/coarse the sampling that
was carried out is. It Is the measure of the smallest discernible detail in an image.

  - A Widely used definition of image resolution is the largest number of 
  discernible line pairs per unit distance

  - Dots per unit distance is a measure of image resolution used commonly in printing
  and publishing. A Common unit is **Dots per Inch**(DPI)

  - We can also measure resolution using a pair of values representing how 
  many pixels there are in a row and how many pixels there are in a column.
  

- Saturation : The Value beyond which all intensity levels are clipped
(changed to the same level).

- Image File Format : There are many image file formats(gif,png,jpeg). They 
Each have their own advantages and disadvantages. 

### Image Interpolation

Image interpolation is the use of known data to estimate values 
at unknown location. it is an important tool in zooming, shrinking,
shrinking, rotation , and geometric connections.
#### Zooming 

There are several interpolation for zooming. After a new grid 
the size of the new resolution is generated, the missing intensities can 
be estimated by these methods :

  - Nearest Neighbor : Assigns each new location the intensity of its 
  nearest neighbor in th original image.
    - Simple.
    - Quick.
    - However, generates distortion of straight edges.
    - Not used frequently.
    
  - Bilinear : Use the four nearest pixels to estimate the intensity of the
   new location. [This method is described in detail here](https://en.wikipedia.org/wiki/Bilinear_interpolation#Application_in_image_processing).
    
  - Bicubic : Use the nearest sixteen neighbors of a point to determine
  its intensity. [Bicubic interpolation is described in detail here](https://en.wikipedia.org/wiki/Bicubic_interpolation#Use_in_computer_graphics).
